{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install defusedxml\n",
    "#%pip install openpyxl\n",
    "#%pip install scikit-bio\n",
    "#%pip install scikit-learn\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skbio.stats.composition import clr, multiplicative_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Reference Columns with Validation Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: File can be found at /manitou/pmg/projects/korem_lab/Data/Freedberg_inulin_trial/validation_data/metadata\n",
    "val_ref = pd.read_excel(\"/Users/mcarrion/Korem_Lab/ICU_Reference.xlsx\")\n",
    "val_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_abundance = pd.read_csv(\"/Users/mcarrion/Korem_Lab/combined/filtered_seqtab_overlap.csv\",index_col=0)\n",
    "val_abundance = pd.read_csv(\"/Users/mcarrion/Korem_Lab/scrub_results/unified_scrubbed_counts.csv\",index_col=0)\n",
    "val_abundance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot ra table\n",
    "# Reset index to make sample names (SB1, SB2, ...) a column\n",
    "val_abundance = val_abundance.reset_index().rename(columns={\"index\": \"sample\"})\n",
    "val_abundance = val_abundance[val_abundance[\"sample\"].str.startswith(\"val_\")]\n",
    "val_abundance[\"sample\"] = val_abundance[\"sample\"].str.split(\"_\").str[1]\n",
    "\n",
    "# Remove/Rename erroneous samples\n",
    "val_abundance.loc[val_abundance[\"sample\"] == \"BS199\", \"sample\"] = \"SB199\"\n",
    "val_abundance = val_abundance[~val_abundance[\"sample\"].astype(str).str.startswith((\"PCR\", \"Ext\", \"Pos\",\"Unnamed\"))]\n",
    "\n",
    "val_abundance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns containing sample numbers\n",
    "sample_cols = [col for col in val_ref.columns if col.startswith(\"samplenumber\")]\n",
    "\n",
    "# Melt the reference DataFrame while keeping all other columns\n",
    "val_ref_long = val_ref.melt(\n",
    "    id_vars=[col for col in val_ref.columns if col not in sample_cols],  # Keep all non-sample columns\n",
    "    value_vars=sample_cols,  \n",
    "    var_name=\"sample_type\",  \n",
    "    value_name=\"sample\"\n",
    ")\n",
    "\n",
    "# Show the structure of the reshaped ref\n",
    "val_ref_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge based on the sample column\n",
    "merged_df_val = val_abundance.merge(val_ref_long, on=\"sample\", how=\"left\")\n",
    "\n",
    "# Show the final merged structure\n",
    "merged_df_val.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add binary death and infection columns\n",
    "merged_df_val[\"death\"] = merged_df_val[\"Date of death\"].notnull().astype(int)\n",
    "merged_df_val[\"infection\"] = merged_df_val[\"infectiondate1\"].notnull().astype(int)\n",
    "merged_df_val = merged_df_val.rename(columns={\"sample\": \"Sample\"})\n",
    "merged_df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Reference Columns with Original Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_abundance = pd.read_csv(\"/Users/mcarrion/Korem_Lab/combined/filtered_seqtab_overlap.csv\",index_col=0)\n",
    "orig_abundance = pd.read_csv(\"/Users/mcarrion/Korem_Lab/scrub_results/unified_scrubbed_counts.csv\",index_col=0)\n",
    "orig_abundance = orig_abundance.reset_index().rename(columns={\"index\": \"sample\"})\n",
    "orig_abundance = orig_abundance[orig_abundance[\"sample\"].str.startswith(\"orig_\")]\n",
    "orig_abundance[\"sample\"] = orig_abundance[\"sample\"].str.removeprefix(\"orig_\")\n",
    "orig_abundance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB: File can be found at /manitou/pmg/projects/korem_lab/Data/Freedberg_inulin_trial/metadata\n",
    "orig_ref = pd.read_excel(\"/Users/mcarrion/Korem_Lab/orig_data_outcomes.xls\")\n",
    "orig_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the part before the dash in \"Sample\"\n",
    "orig_abundance[\"id\"] = orig_abundance[\"sample\"].str.split(\"-\").str[0]\n",
    "\n",
    "# Step 2: Merge the tables on \"id\"\n",
    "merged_df_orig = orig_abundance.merge(orig_ref, on=\"id\", how=\"inner\")\n",
    "merged_df_orig[\"sample\"] = merged_df_orig[\"sample\"].str.split(\"_\").str[0]\n",
    "\n",
    "# Display merged DataFrame\n",
    "merged_df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_orig.to_pickle(\"/Users/mcarrion/Korem_Lab/combined/merged_df_orig.pkl\")\n",
    "merged_df_val.to_pickle(\"/Users/mcarrion/Korem_Lab/combined/merged_df_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in SOFA Scores (& other feature engineering) - For original cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: File can be found at /manitou/pmg/projects/korem_lab/Data/Freedberg_inulin_trial/metadata\n",
    "sofa = pd.read_csv('/burg/pmg/users/mc5672/prev_analysis/data/metadata/sofascores.csv', index_col=0)\n",
    "sofa_scores = sofa[['sofascore']]\n",
    "merged_df_orig = merged_df_orig.merge(\n",
    "    sofa_scores,\n",
    "    how='left',\n",
    "    left_on='sample',\n",
    "    right_index=True\n",
    ")\n",
    "merged_df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_orig['day'] = merged_df_orig['sample'].str.extract(r'-D(\\d+)').astype(int)\n",
    "merged_df_orig['death_next10'] = (\n",
    "    (merged_df_orig['death_day'] >= merged_df_orig['day']) &\n",
    "    (merged_df_orig['death_day'] <= merged_df_orig['day'] + 10)\n",
    ")\n",
    "merged_df_orig['death_next7'] = (\n",
    "    (merged_df_orig['death_day'] >= merged_df_orig['day']) &\n",
    "    (merged_df_orig['death_day'] <= merged_df_orig['day'] + 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to balance data?\n",
    "merged_df_orig['death_next10'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_orig['days_since_icu'] = merged_df_orig['sample'].str.extract(r'-D(\\d+)').astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in Sofa scores (& other feature engineering) - For validation cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sofa(row):\n",
    "    score = 0\n",
    "\n",
    "    # Respiratory (PaO2/FiO2)\n",
    "    try:\n",
    "        pfr = row['Sofa_PaO2'] / (row['Sofa_FiO2'] / 100)\n",
    "        if pfr < 100 and row.get('Vent (Y=1, No=2)') == 1:\n",
    "            score += 4\n",
    "        elif pfr < 200 and row.get('Vent (Y=1, No=2)') == 1:\n",
    "            score += 3\n",
    "        elif pfr < 300:\n",
    "            score += 2\n",
    "        elif pfr < 400:\n",
    "            score += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Coagulation\n",
    "    plts = row.get('Sofa_platelets')\n",
    "    if pd.notnull(plts):\n",
    "        if plts < 20:\n",
    "            score += 4\n",
    "        elif plts < 50:\n",
    "            score += 3\n",
    "        elif plts < 100:\n",
    "            score += 2\n",
    "        elif plts < 150:\n",
    "            score += 1\n",
    "\n",
    "    # Liver\n",
    "    bili = row.get('Sofa_bilirubin')\n",
    "    if pd.notnull(bili):\n",
    "        if bili >= 12:\n",
    "            score += 4\n",
    "        elif bili >= 6:\n",
    "            score += 3\n",
    "        elif bili >= 2:\n",
    "            score += 2\n",
    "        elif bili >= 1.2:\n",
    "            score += 1\n",
    "\n",
    "    # Cardiovascular\n",
    "    pressor = row.get('Pressors (Low dose=1, Medium=2, High=3)')\n",
    "    if pd.notnull(pressor):\n",
    "        if pressor == 3:\n",
    "            score += 4\n",
    "        elif pressor == 2:\n",
    "            score += 3\n",
    "        elif pressor == 1:\n",
    "            score += 2\n",
    "    else:\n",
    "        if row.get('MAP < 70 (Yes=1, No=2)') == 1:\n",
    "            score += 1\n",
    "\n",
    "    # CNS\n",
    "    gcs = row.get('GCS ')\n",
    "    if pd.notnull(gcs):\n",
    "        if gcs < 6:\n",
    "            score += 4\n",
    "        elif gcs < 9:\n",
    "            score += 3\n",
    "        elif gcs < 12:\n",
    "            score += 2\n",
    "        elif gcs < 15:\n",
    "            score += 1\n",
    "\n",
    "    # Renal\n",
    "    cr = row.get('Sofa_creatinine')\n",
    "    if pd.notnull(cr):\n",
    "        if cr >= 5.0:\n",
    "            score += 4\n",
    "        elif cr >= 3.5:\n",
    "            score += 3\n",
    "        elif cr >= 2.0:\n",
    "            score += 2\n",
    "        elif cr >= 1.2:\n",
    "            score += 1\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_sample_date(row):\n",
    "    if pd.notnull(row[\"sample_type\"]):\n",
    "        match = re.search(r'(\\d+)', row[\"sample_type\"])\n",
    "        if match:\n",
    "            sample_num = match.group(1)\n",
    "            col_name = f'sampledate{sample_num}'\n",
    "            if col_name in merged_df_val.columns:\n",
    "                return row[col_name]\n",
    "    return np.nan\n",
    "\n",
    "merged_df_val['date_of_sample'] = merged_df_val.apply(get_sample_date, axis=1)\n",
    "\n",
    "merged_df_val['sofascore'] = merged_df_val.apply(compute_sofa, axis=1)\n",
    "\n",
    "# Make sure both columns are datetime\n",
    "merged_df_val['Date of death'] = pd.to_datetime(merged_df_val['Date of death'], errors='coerce')\n",
    "merged_df_val['date_of_sample'] = pd.to_datetime(merged_df_val['date_of_sample'], errors='coerce')\n",
    "\n",
    "# Compute days until death\n",
    "merged_df_val['days_until_death'] = (merged_df_val['Date of death'] - merged_df_val['date_of_sample']).dt.days\n",
    "\n",
    "# Create binary outcome: death within 10 days\n",
    "merged_df_val['death_next10'] = merged_df_val['days_until_death'].between(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_val['date_of_sample'] = pd.to_datetime(merged_df_val['date_of_sample'])\n",
    "merged_df_val['Date of ICU admission'] = pd.to_datetime(merged_df_val['Date of ICU admission'])\n",
    "\n",
    "# Calculate how many days after ICU admission the sample was taken\n",
    "merged_df_val['days_since_icu'] = (merged_df_val['date_of_sample'] - merged_df_val['Date of ICU admission']).dt.days\n",
    "merged_df_val = merged_df_val.dropna(subset=['days_since_icu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLR Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of shared ASVs between the two cohorts\n",
    "features = pd.read_csv(\"/manitou/pmg/users/mc5672/post_processing_data/unified_scrubbed_counts.csv\",index_col=0).columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AbundanceScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting needed; just keep column names for consistency\n",
    "        self.feature_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.div(X.sum(axis=1), axis=0)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names\n",
    "\n",
    "class LogRAScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        rel_abundance = X.div(X.sum(axis=1), axis=0)\n",
    "        return np.log1p(rel_abundance)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PredictionPipelineV3.PipelineSteps.Preprocessor import CLRScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set this to one of: 'clr', 'abun', 'log_ra'\n",
    "scaler = 'clr'\n",
    "\n",
    "# Set index\n",
    "merged_df_orig = merged_df_orig.set_index('id')\n",
    "X_day = merged_df_orig[['day']].copy()\n",
    "\n",
    "# Split features\n",
    "abundance_features = [f for f in features if f.lower() != 'sofascore']\n",
    "sofa_feature = ['sofascore']\n",
    "X_abundance = merged_df_orig[abundance_features].copy()\n",
    "if scaler == 'clr':\n",
    "\t#X_abundance = np.log1p(X_abundance.div(X_abundance.sum(axis=1), axis=0))  #convert to log_rel_abundance\n",
    "\tX_abundance = X_abundance.div(X_abundance.sum(axis=1), axis=0)  #convert to rel_abundance\n",
    "\n",
    "X_sofa = merged_df_orig[sofa_feature].copy()\n",
    "\n",
    "# Map scaler string to actual scaler class\n",
    "scaler_map = {\n",
    "    'clr': CLRScaler,\n",
    "    'abun': AbundanceScaler,\n",
    "    'log_ra': LogRAScaler\n",
    "}\n",
    "\n",
    "# Initialize and apply selected scaler\n",
    "if scaler not in scaler_map:\n",
    "    raise ValueError(f\"Unknown scaler type: {scaler}\")\n",
    "\n",
    "abundance_scaler = scaler_map[scaler]()\n",
    "X_abundance_proc = pd.DataFrame(\n",
    "    abundance_scaler.fit_transform(X_abundance),\n",
    "    index=X_abundance.index,\n",
    "    columns=X_abundance.columns\n",
    ")\n",
    "\n",
    "# Preprocess SOFA\n",
    "sofa_scaler = StandardScaler()\n",
    "X_sofa_proc = pd.DataFrame(\n",
    "    sofa_scaler.fit_transform(X_sofa),\n",
    "    index=X_sofa.index,\n",
    "    columns=X_sofa.columns\n",
    ")\n",
    "\n",
    "# Combine features\n",
    "X_new = pd.concat([X_abundance_proc, X_sofa_proc, X_day], axis=1)\n",
    "X_new = X_new.rename(columns={'day': 'days_since_icu'})\n",
    "\n",
    "# Recombine with metadata\n",
    "merged_meta = merged_df_orig.drop(columns=X_new.columns, errors='ignore')\n",
    "merged_df_orig_processed = pd.concat([merged_meta, X_new], axis=1).reset_index(drop=False)\n",
    "\n",
    "# Save if needed\n",
    "#output_path = f\"/manitou/pmg/users/mc5672/post_processing_data/merged_df_orig_{scaler}_logra.csv\"\n",
    "#merged_df_orig_processed.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply same values to validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Sample' is set as index for matching row alignment\n",
    "merged_df_val = merged_df_val.set_index('Sample')\n",
    "\n",
    "# Define abundance and sofa features\n",
    "abundance_features = [f for f in features if f.lower() != 'sofascore']\n",
    "sofa_feature = ['sofascore']\n",
    "X_abundance_val = merged_df_val[abundance_features].copy()\n",
    "if scaler == 'clr':\n",
    "\t# X_abundance_val = np.log1p(X_abundance_val.div(X_abundance_val.sum(axis=1), axis=0))  #convert to log_rel_abundance\n",
    "\tX_abundance_val = X_abundance_val.div(X_abundance_val.sum(axis=1), axis=0)  #convert to rel_abundance\n",
    "\n",
    "\t\n",
    "X_sofa_val = merged_df_val[sofa_feature].copy()\n",
    "X_day_val = merged_df_val[['days_since_icu']].copy()\n",
    "\n",
    "# Apply the already-fitted scalers from training\n",
    "X_abundance_val_proc = pd.DataFrame(\n",
    "    abundance_scaler.transform(X_abundance_val),\n",
    "    index=X_abundance_val.index,\n",
    "    columns=X_abundance_val.columns\n",
    ")\n",
    "\n",
    "X_sofa_val_proc = pd.DataFrame(\n",
    "    sofa_scaler.transform(X_sofa_val),\n",
    "    index=X_sofa_val.index,\n",
    "    columns=X_sofa_val.columns\n",
    ")\n",
    "\n",
    "# Combine processed validation data\n",
    "X_val_new = pd.concat([X_abundance_val_proc, X_sofa_val_proc, X_day_val], axis=1)\n",
    "\n",
    "# Recombine with metadata, avoiding column duplication\n",
    "val_meta = merged_df_val.drop(columns=X_val_new.columns, errors='ignore')\n",
    "merged_df_val_processed = pd.concat([val_meta, X_val_new], axis=1).reset_index(drop=False)\n",
    " \n",
    "# Optionally save to CSV\n",
    "#output_path_val = f\"/manitou/pmg/users/mc5672/post_processing_data/merged_df_val_{scaler}_for_testing.csv\"\n",
    "#merged_df_val_processed.to_csv(output_path_val, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

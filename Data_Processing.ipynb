{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Raw Data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DADA2 (v1.8) ###\n",
    "https://benjjneb.github.io/dada2/tutorial_1_8.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(dada2); packageVersion(\"dada2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "path <- \"/Users/mcarrion/Korem_Lab/combined/data\" \n",
    "list.files(path)\n",
    "\n",
    "# files should be of the form:\n",
    "# orig_1001A-D0_S25_1.fastq\n",
    "# orig_1001A-D0_S25_2.fastq\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Forward and reverse fastq filenames have format: _1.fastq and _2.fastq, respectively\n",
    "fnFs <- sort(list.files(path, pattern=\"_1.fastq\", full.names = TRUE))\n",
    "fnRs <- sort(list.files(path, pattern=\"_2.fastq\", full.names = TRUE))\n",
    "\n",
    "# Extract sample names\n",
    "sample.names <- sapply(strsplit(sub(\"\\\\.fastq$|\\\\.fq$\", \"\", basename(fnFs)), \"_\"), function(x) {\n",
    "  # If the last element is \"1\" or \"2\", remove it\n",
    "  if (x[length(x)] %in% c(\"1\", \"2\")) {\n",
    "    x <- x[-length(x)]  # Drop last element\n",
    "  }\n",
    "  return(paste(x, collapse = \"_\"))  # Reconstruct name\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plotQualityProfile(fnFs[21:22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Place filtered files in filtered/ subdirectory\n",
    "filtFs <- file.path(path, \"filtered\", paste0(sample.names, \"_F_filt.fastq.gz\"))\n",
    "filtRs <- file.path(path, \"filtered\", paste0(sample.names, \"_R_filt.fastq.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(280,240),\n",
    "              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=FALSE,\n",
    "              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE\n",
    "head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "errF <- learnErrors(filtFs, multithread=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "errR <- learnErrors(filtRs, multithread=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plotErrors(errF, nominalQ=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "derepFs <- derepFastq(filtFs, verbose=TRUE)\n",
    "names(derepFs) <- sample.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "derepRs <- derepFastq(filtRs, verbose=TRUE)\n",
    "names(derepRs) <- sample.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dadaFs <- dada(derepFs, err=errF, multithread=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dadaRs <- dada(derepRs, err=errR, multithread=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)\n",
    "\n",
    "# Inspect the merger data.frame from the first sample\n",
    "head(mergers[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "seqtab <- makeSequenceTable(mergers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=TRUE, verbose=TRUE)\n",
    "dim(seqtab.nochim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "getN <- function(x) sum(getUniques(x))\n",
    "track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))\n",
    "# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)\n",
    "colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\n",
    "rownames(track) <- sample.names\n",
    "head(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# filtered_seqtab obtained from below\n",
    "# in reality, we just get colnames/rownames, so can do it using seqtab.nochim above too\n",
    "filtered_seqtab2 <- filtered_seqtab %>%\n",
    "\tselect(-any_of(c(\"Sample_ID\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")))\n",
    "rownames(filtered_seqtab2) <- filtered_seqtab2[, \"Sample\"]  # Assign sample names as row names\n",
    "filtered_seqtab2 <- filtered_seqtab2[, -1, drop=FALSE]  # Remove \"Sample\" column\n",
    "filtered_seqtab2 <- t(filtered_seqtab2)\n",
    "head(filtered_seqtab2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# NB: taxonomy data can be downloaded from dada2 link above\n",
    "taxa <- assignTaxonomy(rownames(filtered_seqtab2), \"/Users/mcarrion/Korem_Lab/validation_data/silva_nr_v128_train_set.fa.gz\", multithread=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "taxa <- addSpecies(taxa, \"/Users/mcarrion/Korem_Lab/validation_data/silva_species_assignment_v128.fa.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "path <- file.path(\"/Users/mcarrion/Korem_Lab/combined\")\n",
    "\n",
    "write.csv(taxa, file = file.path(path, \"taxa.csv\"), row.names=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "taxa.print <- taxa # Removing sequence rownames for display only\n",
    "rownames(taxa.print) <- NULL\n",
    "head(taxa.print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "path <- file.path(\"/Users/mcarrion/Korem_Lab/combined\")\n",
    "\n",
    "# Save CSV files in the specified folder\n",
    "write.csv(seqtab.nochim, file = file.path(path, \"seqtab_nochim.csv\"), row.names=TRUE)\n",
    "write.csv(track, file = file.path(path, \"track.csv\"), row.names=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "# ** Read and fix the track file**\n",
    "read_and_fix_track <- function(filepath) {\n",
    "  df <- read.csv(filepath, check.names = FALSE)  # Read CSV without altering names\n",
    "\n",
    "  # Ensure the first column is named \"Sample\"\n",
    "  if (colnames(df)[1] == \"\" || colnames(df)[1] == \"X\") {\n",
    "    colnames(df)[1] <- \"Sample\"\n",
    "  }\n",
    "\n",
    "  return(df)\n",
    "}\n",
    "\n",
    "# Read track.csv from orig_data directory\n",
    "#track_combined <- read_and_fix_track(\"/Users/mcarrion/Korem_Lab/combined/track.csv\")\n",
    "#track_combined <- track\n",
    "\n",
    "# ** Extract Sample ID (everything before first `_`)**\n",
    "track_combined <- track_combined %>%\n",
    "  mutate(\n",
    "    Sample_ID = str_extract(Sample, \"^[^_]+_[^_]+\")  # Extract part before first `_`\n",
    "  )\n",
    "\n",
    "# ** Keep only the row with the highest 'nonchim' value per Sample_ID**\n",
    "best_samples <- track_combined %>%\n",
    "  group_by(Sample_ID) %>%\n",
    "  slice_max(nonchim, with_ties = FALSE) %>%\n",
    "  ungroup()\n",
    "\n",
    "print(\"Best samples selected successfully!\")\n",
    "print(best_samples)\n",
    "\n",
    "# ** Read and fix the sequence table**\n",
    "#seqtab <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/seqtab_nochim.csv\",\n",
    "#                   row.names = 1, check.names = FALSE) %>%\n",
    "#  rownames_to_column(\"Sample\")\n",
    "\n",
    "# ** Extract Sample ID from Sequence Table**\n",
    "seqtab <- seqtab %>%\n",
    "  mutate(\n",
    "    Sample_ID = str_extract(Sample, \"^[^_]+_[^_]+\")  # Extract part before first `_`\n",
    "  )\n",
    "\n",
    "# ** Filter sequence table to keep only the best Sample_IDs**\n",
    "filtered_seqtab <- seqtab %>%\n",
    "  filter(Sample %in% best_samples$Sample)\n",
    "\n",
    "# **Ensure \"Sample\" is the identifier column**\n",
    "colnames(filtered_seqtab)[1] <- \"Sample\"\n",
    "\n",
    "# **Save the deduplicated sequence table**\n",
    "#write.csv(filtered_seqtab, \"/Users/mcarrion/Korem_Lab/orig_data/deduplicated_seqtab_nochim.csv\", row.names = FALSE)\n",
    "#print(\"Filtered sequence table saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Erroneous Samples ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "filtered_seqtab <- filtered_seqtab %>% filter(Sample != \"val_SB157_S244\")\n",
    "# print(\"Checking if SB157 still exists in filtered_seqtab:\")\n",
    "print(\"val_SB157_S244\" %in% filtered_seqtab$Sample)  # Should return FALSE\n",
    "# write.csv(filtered_seqtab, \"/Users/mcarrion/Korem_Lab/data/deduplicated_seqtab_nochim.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Remove extra columns used for de-duplication \n",
    "#filtered_seqtab <- filtered_seqtab %>%\n",
    "#  select(-any_of(c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")))\n",
    "#write.csv(filtered_seqtab, \"/Users/mcarrion/Korem_Lab/data/deduplicated_seqtab_nochim.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how much overlap exists between two cohorts ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "filtered_seqtab <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/deduplicated_seqtab_nochim.csv\",\n",
    "                  row.names = 1, check.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "filtered_seqtab <- filtered_seqtab[, !colnames(filtered_seqtab) %in% c(\"Sample_ID\"), drop = FALSE]\n",
    "\n",
    "head(filtered_seqtab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract row names (sample names)\n",
    "sample_names <- rownames(filtered_seqtab)\n",
    "\n",
    "# Identify orig_ and val_ sample rows\n",
    "orig_rows <- grep(\"^orig_\", sample_names, value = TRUE)\n",
    "val_rows  <- grep(\"^val_\", sample_names, value = TRUE)\n",
    "\n",
    "# Subset ASV abundances for orig_ and val_ samples (keeping only numeric ASV columns)\n",
    "orig_abundance <- filtered_seqtab[orig_rows, , drop = FALSE]\n",
    "val_abundance  <- filtered_seqtab[val_rows, , drop = FALSE]\n",
    "\n",
    "# Ensure all ASV columns are numeric (in case of any issues)\n",
    "orig_abundance[] <- lapply(orig_abundance, as.numeric)\n",
    "val_abundance[]  <- lapply(val_abundance, as.numeric)\n",
    "\n",
    "# Identify ASVs present in at least one sample in each group\n",
    "asvs_in_orig <- colSums(orig_abundance > 0) > 0\n",
    "asvs_in_val  <- colSums(val_abundance > 0) > 0\n",
    "\n",
    "# Find ASVs unique to each group\n",
    "unique_asvs_orig <- asvs_in_orig & !asvs_in_val  # ASVs in orig_ but NOT in val_\n",
    "unique_asvs_val <- asvs_in_val & !asvs_in_orig  # ASVs in val_ but NOT in orig_\n",
    "\n",
    "# Compute total read counts for each unique ASV\n",
    "reads_unique_orig <- colSums(orig_abundance[, unique_asvs_orig, drop = FALSE])\n",
    "reads_unique_val  <- colSums(val_abundance[, unique_asvs_val, drop = FALSE])\n",
    "\n",
    "# Compute total reads for entire orig_ and val_ groups\n",
    "total_reads_orig <- sum(orig_abundance)\n",
    "total_reads_val  <- sum(val_abundance)\n",
    "\n",
    "# Compute proportion of total reads from unique ASVs\n",
    "prop_reads_unique_orig <- sum(reads_unique_orig) / total_reads_orig\n",
    "prop_reads_unique_val  <- sum(reads_unique_val) / total_reads_val\n",
    "\n",
    "# Compute proportion of each unique ASV relative to total reads in orig_/val_\n",
    "prop_each_unique_orig <- reads_unique_orig / total_reads_orig\n",
    "prop_each_unique_val  <- reads_unique_val / total_reads_val\n",
    "\n",
    "# Sort ASVs by highest proportion\n",
    "sorted_orig <- sort(prop_each_unique_orig, decreasing = TRUE)\n",
    "sorted_val <- sort(prop_each_unique_val, decreasing = TRUE)\n",
    "\n",
    "# Convert to data frames for easy viewing\n",
    "top_unique_orig <- data.frame(ASV = names(sorted_orig), Proportion = sorted_orig)\n",
    "top_unique_val <- data.frame(ASV = names(sorted_val), Proportion = sorted_val)\n",
    "\n",
    "# Print total proportion of unique ASVs\n",
    "cat(\"Proportion of total reads from unique ASVs in orig_:\", prop_reads_unique_orig, \"\\n\")\n",
    "cat(\"Proportion of total reads from unique ASVs in val_:\", prop_reads_unique_val, \"\\n\")\n",
    "\n",
    "# Print top 10 ASVs contributing the most reads among unique ASVs\n",
    "cat(\"\\nTop 10 ASVs unique to orig_ samples (as proportion of total orig_ reads):\\n\")\n",
    "print(head(top_unique_orig, 10))\n",
    "\n",
    "cat(\"\\nTop 10 ASVs unique to val_ samples (as proportion of total val_ reads):\\n\")\n",
    "print(head(top_unique_val, 10))\n",
    "\n",
    "overlapping_asvs <- asvs_in_orig & asvs_in_val  # Logical vector for intersecting ASVs\n",
    "\n",
    "# Create a new table with only overlapping ASVs\n",
    "filtered_seqtab_overlap <- filtered_seqtab[, overlapping_asvs, drop = FALSE]\n",
    "\n",
    "# Compute total reads in full dataset and in overlapping ASVs\n",
    "total_reads_all <- sum(filtered_seqtab)  # Sum all ASV reads\n",
    "total_reads_overlap <- sum(filtered_seqtab_overlap)  # Sum reads from intersecting ASVs\n",
    "\n",
    "# Compute proportion of reads from intersecting ASVs\n",
    "prop_reads_overlap <- total_reads_overlap / total_reads_all\n",
    "\n",
    "# Print results\n",
    "cat(\"Dimensions of filtered_seqtab:\", dim(filtered_seqtab), \"\\n\")\n",
    "cat(\"Dimensions of filtered_seqtab_overlap:\", dim(filtered_seqtab_overlap), \"\\n\")\n",
    "cat(\"Proportion of overlapping ASVs:\", dim(filtered_seqtab_overlap)[2] / dim(filtered_seqtab)[2], \"\\n\")\n",
    "cat(\"Proportion of total reads from overlapping ASVs:\", prop_reads_overlap, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cat(\"ASVs in Orig:\", sum(asvs_in_orig), \"\\n\")\n",
    "cat(\"ASVs in Val:\",sum(asvs_in_val), \"\\n\")\n",
    "cat(\"ASVs in Both:\",sum(overlapping_asvs), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate how many samples each ASV is present in\n",
    "asv_presence_counts <- colSums(filtered_seqtab_overlap > 0)\n",
    "\n",
    "# Determine threshold: 10% of total samples\n",
    "sample_threshold <- 0.10 * nrow(filtered_seqtab_overlap)\n",
    "\n",
    "# Count how many ASVs are present in at least 10% of samples\n",
    "num_asvs_10pct <- sum(asv_presence_counts >= sample_threshold)\n",
    "\n",
    "# Print result\n",
    "num_asvs_10pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#head(filtered_seqtab_overlap)\n",
    "#write.csv(filtered_seqtab_overlap, \"/Users/mcarrion/Korem_Lab/combined/filtered_seqtab_overlap.csv\", row.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "# Compute total reads per ASV across all samples\n",
    "total_reads_per_asv <- colSums(filtered_seqtab)\n",
    "\n",
    "# Compute proportion of total reads per ASV\n",
    "prop_reads_per_asv <- total_reads_per_asv / sum(total_reads_per_asv)\n",
    "\n",
    "# Create a data frame for plotting\n",
    "asv_data <- data.frame(\n",
    "  ASV = names(prop_reads_per_asv),\n",
    "  Proportion = prop_reads_per_asv,\n",
    "  Overlap_Status = ifelse(names(prop_reads_per_asv) %in% names(overlapping_asvs[overlapping_asvs]), \n",
    "                          \"Overlapping\", \"Unique\")\n",
    ")\n",
    "\n",
    "# Sort by proportion for better visualization\n",
    "asv_data <- asv_data[order(-asv_data$Proportion), ]\n",
    "\n",
    "# Generate scatter plot\n",
    "ggplot(asv_data, aes(x = seq_along(ASV), y = Proportion, color = Overlap_Status)) +\n",
    "  geom_point(alpha = 0.7) +  # Scatter points with transparency\n",
    "  scale_y_log10() +  # Log scale to highlight differences in abundance\n",
    "  labs(title = \"Proportion of Total Reads per ASV\",\n",
    "       x = \"ASV Rank (Sorted by Abundance)\",\n",
    "       y = \"Proportion of Total Reads (Log Scale)\",\n",
    "       color = \"ASV Type\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Generate box plot\n",
    "ggplot(asv_data, aes(x = Overlap_Status, y = Proportion, fill = Overlap_Status)) +\n",
    "  geom_boxplot(outlier.shape = NA, alpha = 0.6) +  # Box plot with no individual outliers\n",
    "  geom_jitter(width = 0.2, alpha = 0.2) +  # Add slight scatter for visibility\n",
    "  scale_y_log10() +  # Log scale for better visualization\n",
    "  labs(title = \"Distribution of ASV Read Proportions\",\n",
    "       x = \"ASV Type\",\n",
    "       y = \"Proportion of Total Reads (Log Scale)\",\n",
    "       fill = \"ASV Type\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Generate density plot\n",
    "ggplot(asv_data, aes(x = Proportion, fill = Overlap_Status)) +\n",
    "  geom_density(alpha = 0.5, adjust = 1) +  # Density plot with transparency\n",
    "  scale_x_log10() +  # Log scale for better visualization\n",
    "  labs(title = \"Distribution of ASV Read Proportions\",\n",
    "       x = \"Proportion of Total Reads (Log Scale)\",\n",
    "       y = \"Density\",\n",
    "       fill = \"ASV Type\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# library(stringdist)\n",
    "\n",
    "# # Convert logical vectors to ASV lists\n",
    "# asvs_orig <- names(asvs_in_orig[asvs_in_orig])  # Extract ASVs present in orig\n",
    "# asvs_val <- names(asvs_in_val[asvs_in_val])  # Extract ASVs present in val\n",
    "\n",
    "# # Compute pairwise Levenshtein distances between ASVs\n",
    "# dist_matrix <- stringdistmatrix(asvs_orig, asvs_val, method = \"lv\")  # \"lv\" = Levenshtein\n",
    "\n",
    "# # Set mismatch threshold (e.g., X = 2)\n",
    "# X <- 2\n",
    "\n",
    "# # Identify fuzzy matches: Find ASV pairs where the distance is ≤ X\n",
    "# fuzzy_matches <- which(dist_matrix <= X, arr.ind = TRUE)\n",
    "\n",
    "# # Extract matched ASVs\n",
    "# matched_orig <- asvs_orig[fuzzy_matches[, 1]]\n",
    "# matched_val <- asvs_val[fuzzy_matches[, 2]]\n",
    "\n",
    "# # Create a data frame of fuzzy matches\n",
    "# fuzzy_asv_mapping <- data.frame(orig_ASV = matched_orig, val_ASV = matched_val, distance = dist_matrix[fuzzy_matches])\n",
    "\n",
    "# # Print results\n",
    "# cat(\"Number of fuzzy-matched ASVs:\", nrow(fuzzy_asv_mapping), \"\\n\")\n",
    "# head(fuzzy_asv_mapping)  # Show first few matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SCRuB for decontamination ##\n",
    "https://github.com/Shenhav-and-Korem-labs/SCRuB/tree/main/tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check if negative samples have reads (contamination)\n",
    "filtered_seqtab <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/filtered_seqtab_overlap.csv\",row.names = 1, check.names = FALSE)\n",
    "\n",
    "\n",
    "# Subset the rows that match target samples\n",
    "matching_rows <- filtered_seqtab[grep(\"Neg\", rownames(filtered_seqtab),ignore.case = TRUE), , drop = FALSE]\n",
    "\n",
    "# Compute total read counts (sum across all columns)\n",
    "total_reads_per_sample <- rowSums(matching_rows)\n",
    "\n",
    "# Print results\n",
    "print(total_reads_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "devtools::install_github(\"shenhav-and-korem-labs/SCRuB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(stringr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load file list\n",
    "file_list <- readLines(\"/Users/mcarrion/Korem_Lab/file_list.txt\")\n",
    "\n",
    "# Define directories (batches)\n",
    "val_dirs <- c(\"20241029_16S_FB_ICU\", \"20241127_16S_FB_ICU\", \"20241223_16S_FB_ICU\")\n",
    "orig_dirs <- c(\"20231228_16S_Plate_1\", \"20231228_16S_Plate_2\", \"20231228_16S_Plate_3\", \"20240202_16S_FB\")\n",
    "\n",
    "# Initialize empty batch mapping\n",
    "sample_to_batch <- list()\n",
    "\n",
    "\n",
    "clean_sample_name <- function(filename) {\n",
    "  # Remove L001, _R1_001.fastq.gz, _R2_001.fastq.gz, _1.fastq.gz, _2.fastq.gz\n",
    "  filename <- str_remove(filename, \"_L001\")  # Remove _L001 if present\n",
    "  filename <- str_remove(filename, \"_R[12]_001\\\\.fastq\\\\.gz$\")  # Remove _R1_001.fastq.gz, _R2_001.fastq.gz\n",
    "  filename <- str_remove(filename, \"_[12]\\\\.fastq\\\\.gz$\")  # Remove _1.fastq.gz, _2.fastq.gz\n",
    "  return(filename)\n",
    "}\n",
    "\n",
    "# Process file paths\n",
    "for (filepath in file_list) {\n",
    "    # Extract directory and filename\n",
    "    path_parts <- unlist(strsplit(filepath, \"/\"))\n",
    "    dir_name <- path_parts[length(path_parts) - 1]  # Second to last part (batch directory)\n",
    "    filename <- path_parts[length(path_parts)]     # Last part (filename)\n",
    "\n",
    "    # Remove _1/2 and _R1/2_001.fastq.gz from filename\n",
    "    clean_name <- clean_sample_name(filename)\n",
    "    # Determine batch type\n",
    "    if (dir_name %in% val_dirs) {\n",
    "        sample_to_batch[[paste0(\"val_\", clean_name)]] <- dir_name\n",
    "    } else if (dir_name %in% orig_dirs) {\n",
    "        sample_to_batch[[paste0(\"orig_\", clean_name)]] <- dir_name\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert list to named vector\n",
    "sample_to_batch <- unlist(sample_to_batch)\n",
    "\n",
    "# Assign batch labels in filtered_seqtab\n",
    "filtered_seqtab$Batch <- sample_to_batch[rownames(filtered_seqtab)]\n",
    "\n",
    "# Remove rows where batch couldn't be determined\n",
    "print(filtered_seqtab[is.na(filtered_seqtab$Batch), , drop = FALSE])\n",
    "filtered_seqtab <- filtered_seqtab[!is.na(filtered_seqtab$Batch), ]\n",
    "\n",
    "# Split filtered_seqtab into batch-specific DataFrames\n",
    "batch_dfs <- split(filtered_seqtab, filtered_seqtab$Batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(SCRuB)\n",
    "\n",
    "# Initialize a list to store results\n",
    "all_decontaminated_counts <- list()\n",
    "\n",
    "# Function to apply SCRuB to a batch\n",
    "apply_scrub_to_batch <- function(test_batch, batch_name) {\n",
    "  cat(\"\\n Processing batch:\", batch_name, \"\\n\")  # Print batch name before processing\n",
    "\n",
    "  # Convert to matrix, ensuring numeric values\n",
    "  count_matrix <- as.matrix(test_batch)\n",
    "  count_matrix <- count_matrix[, colnames(test_batch) != \"Batch\", drop = FALSE]\n",
    "\n",
    "  # Convert all columns to numeric safely\n",
    "  count_matrix <- as.matrix(apply(count_matrix, 2, as.numeric))\n",
    "  rownames(count_matrix) <- rownames(test_batch)\n",
    "\n",
    "  # Replace NA values (SCRuB does not accept missing values)\n",
    "  count_matrix[is.na(count_matrix)] <- 0  \n",
    "\n",
    "  # Ensure all values are numeric before SCRuB\n",
    "  if (!all(sapply(count_matrix, is.numeric))) {\n",
    "    stop(paste(\"Error: Non-numeric values found in batch:\", batch_name))\n",
    "  }\n",
    "\n",
    "  # Create metadata\n",
    "metadata <- data.frame(\n",
    "  is_control = grepl(\"ext[-_]neg|pcr[-_]neg|(^|[-_])neg\", rownames(count_matrix), ignore.case = TRUE),\n",
    "  sample_type = case_when(\n",
    "    grepl(\"ext[-_]neg\", rownames(count_matrix), ignore.case = TRUE) ~ \"extraction control\",\n",
    "    grepl(\"pcr[-_]neg\", rownames(count_matrix), ignore.case = TRUE) ~ \"pcr control\",\n",
    "    grepl(\"(^|[-_])neg\", rownames(count_matrix), ignore.case = TRUE) ~ \"extraction control\",\n",
    "    TRUE ~ \"sample\"\n",
    "  )\n",
    ")\n",
    "  rownames(metadata) <- rownames(count_matrix)\n",
    "\n",
    "  # Debugging: Print sample type distribution\n",
    "  cat(\"Sample type distribution in batch\", batch_name, \":\\n\")\n",
    "  print(table(metadata$sample_type))\n",
    "\n",
    "  # **Filter `control_order` to only include present control types**\n",
    "  present_controls <- unique(metadata$sample_type[metadata$is_control])\n",
    "  control_order <- intersect(c(\"extraction control\", \"pcr control\"), present_controls)\n",
    "\n",
    "  # **Skip SCRuB if no valid controls exist (e.g., batch `20240202_16S_FB`)**\n",
    "  if (length(control_order) == 0) {\n",
    "    cat(\"Skipping batch:\", batch_name, \"because no valid controls were found.\\n\")\n",
    "    # Store unprocessed counts in final dataset\n",
    "    all_decontaminated_counts[[batch_name]] <<- count_matrix\n",
    "    \n",
    "    return(list(decontaminated_samples = count_matrix, p = rep(NA, nrow(count_matrix))))\n",
    "  }\n",
    "\n",
    "  # Debugging: Print final control order used\n",
    "  cat(\" Using control_order for batch\", batch_name, \":\", control_order, \"\\n\")\n",
    "\n",
    "  # Apply SCRuB\n",
    "  scrub_result <- SCRuB(count_matrix, metadata, control_order = control_order)\n",
    "\n",
    "  # Extract decontaminated counts\n",
    "  decontaminated_counts <- scrub_result$decontaminated_samples\n",
    "\n",
    "  # Store results in global list\n",
    "  all_decontaminated_counts[[batch_name]] <<- decontaminated_counts\n",
    "\n",
    "  # Ensure output directory exists\n",
    "  output_dir <- \"/Users/mcarrion/Korem_Lab/scrub_results/\"\n",
    "  if (!dir.exists(output_dir)) {\n",
    "    dir.create(output_dir, recursive = TRUE)\n",
    "  }\n",
    "\n",
    "  # Save batch-specific decontaminated matrix\n",
    "  write.csv(decontaminated_counts, file = paste0(output_dir, batch_name, \"_scrubbed.csv\"), row.names = TRUE)\n",
    "\n",
    "  # Save estimated contamination fractions\n",
    "  write.csv(scrub_result$p, file = paste0(output_dir, batch_name, \"_scrub_p.csv\"), row.names = TRUE)\n",
    "\n",
    "  return(scrub_result)  # Return SCRuB result for further analysis if needed\n",
    "}\n",
    "\n",
    "# Apply SCRuB to each batch in batch_dfs, skipping batches without controls\n",
    "scrub_results <- lapply(names(batch_dfs), function(batch) {\n",
    "  result <- apply_scrub_to_batch(batch_dfs[[batch]], batch)\n",
    "  if (!is.null(result)) result  # Only keep non-null results\n",
    "})\n",
    "\n",
    "# Remove NULL values from `scrub_results`\n",
    "scrub_results <- scrub_results[!sapply(scrub_results, is.null)]\n",
    "\n",
    "\n",
    "# Convert results to a named list\n",
    "names(scrub_results) <- names(batch_dfs)[names(batch_dfs) %in% names(scrub_results)]\n",
    "\n",
    "# **Merge all decontaminated counts into one DataFrame**\n",
    "if (length(all_decontaminated_counts) > 0) {\n",
    "  unified_decontaminated_counts <- do.call(rbind, all_decontaminated_counts)\n",
    "\n",
    "  # Save the unified decontaminated DataFrame\n",
    "  write.csv(unified_decontaminated_counts, \"/Users/mcarrion/Korem_Lab/scrub_results/unified_scrubbed_counts.csv\", row.names = TRUE)\n",
    "\n",
    "  cat(\"SCRuB processing complete for all batches!\\n\")\n",
    "} else {\n",
    "  cat(\"No valid batches processed through SCRuB.\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "# Function 1: Plot Estimated Proportion of Non-Contaminated Reads (p values)\n",
    "plot_p_values <- function(scrub_results) {\n",
    "  # Extract valid p values, skipping batches with all NAs\n",
    "  p_values_list <- lapply(scrub_results, function(x) {\n",
    "    if (!is.null(x$p) && length(x$p) > 0 && !all(is.na(x$p))) {\n",
    "      data.frame(Sample = names(x$p), p = unlist(x$p), stringsAsFactors = FALSE)\n",
    "    } else {\n",
    "      return(NULL)  # Skip batches with all NAs\n",
    "    }\n",
    "  })\n",
    "\n",
    "  # Combine non-null results\n",
    "  p_values <- do.call(rbind, p_values_list)\n",
    "\n",
    "  # Ensure we have valid data to plot\n",
    "  if (is.null(p_values) || nrow(p_values) == 0) {\n",
    "    stop(\"⚠️ No valid p values available for plotting.\")\n",
    "  }\n",
    "\n",
    "  # Generate boxplot\n",
    "  ggplot(p_values, aes(x = \"\", y = p)) +\n",
    "    geom_boxplot(fill = \"blue\", alpha = 0.5) +\n",
    "    geom_jitter(width = 0.1, alpha = 0.6, color = \"black\") +\n",
    "    labs(\n",
    "      title = \"Proportion of Sample NOT Identified as Contamination (p values)\", \n",
    "      y = \"Proportion of Sample Retained (p)\", x = \"Samples\"\n",
    "    ) +\n",
    "    theme_minimal()\n",
    "}\n",
    "\n",
    "# Function 2: Plot Total Reads Before vs. After SCRuB\n",
    "plot_total_reads <- function(raw_counts, decontaminated_counts) {\n",
    "  # Ensure row names (samples) match in both datasets\n",
    "  common_samples <- intersect(rownames(raw_counts), rownames(decontaminated_counts))\n",
    "\n",
    "  # Subset both datasets to include only common samples\n",
    "  raw_counts <- raw_counts[common_samples, , drop = FALSE]\n",
    "  decontaminated_counts <- decontaminated_counts[common_samples, , drop = FALSE]\n",
    "\n",
    "  # Compute total reads per sample before & after SCRuB\n",
    "  total_reads_before <- rowSums(raw_counts, na.rm = TRUE)\n",
    "  total_reads_after <- rowSums(decontaminated_counts, na.rm = TRUE)\n",
    "\n",
    "  # Create dataframe for ggplot\n",
    "  df <- data.frame(\n",
    "    Sample = rep(common_samples, 2),\n",
    "    Total_Reads = c(total_reads_before, total_reads_after),\n",
    "    Status = rep(c(\"Before SCRuB\", \"After SCRuB\"), each = length(common_samples))\n",
    "  )\n",
    "\n",
    "  # Ensure we have valid data to plot\n",
    "  if (nrow(df) == 0) {\n",
    "    stop(\"No valid read count data available for plotting.\")\n",
    "  }\n",
    "\n",
    "  # Generate bar plot\n",
    "  ggplot(df, aes(x = Sample, y = Total_Reads, fill = Status)) +\n",
    "    geom_bar(stat = \"identity\", position = \"dodge\") +\n",
    "    labs(title = \"Total Reads Before and After SCRuB\", y = \"Total Reads\", x = \"Samples\") +\n",
    "    theme_minimal() +\n",
    "    theme(axis.text.x = element_blank())  # Hide x-axis labels if too many samples\n",
    "}\n",
    "\n",
    "# Merge & Clean Data for Plotting\n",
    "\n",
    "# Merge all raw batch data\n",
    "raw_counts <- do.call(rbind, batch_dfs)\n",
    "\n",
    "# Extract only valid decontaminated samples\n",
    "valid_decontaminated <- lapply(scrub_results, function(x) {\n",
    "  if (!is.null(x$decontaminated_samples) && nrow(x$decontaminated_samples) > 0) {\n",
    "    return(x$decontaminated_samples)\n",
    "  } else {\n",
    "    return(NULL)\n",
    "  }\n",
    "})\n",
    "\n",
    "# Merge valid decontaminated datasets\n",
    "decontaminated_counts <- do.call(rbind, valid_decontaminated)\n",
    "\n",
    "# Remove \"Batch\" column if it exists\n",
    "raw_counts <- raw_counts[, !colnames(raw_counts) %in% \"Batch\", drop = FALSE]\n",
    "rownames(raw_counts) <- sub(\".*?\\\\.(orig_|val_)\", \"\\\\1\", rownames(raw_counts))\n",
    "\n",
    "decontaminated_counts <- decontaminated_counts[, !colnames(decontaminated_counts) %in% \"Batch\", drop = FALSE]\n",
    "\n",
    "# Run Plots\n",
    "plot_p_values(scrub_results)  # Contamination fractions\n",
    "plot_total_reads(raw_counts, decontaminated_counts)  # Reads before vs after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_proportion_reads_removed <- function(raw_counts, decontaminated_counts) {\n",
    "  # Ensure row names (samples) match in both datasets\n",
    "  common_samples <- intersect(rownames(raw_counts), rownames(decontaminated_counts))\n",
    "\n",
    "  # Skip plotting if no valid samples exist\n",
    "  if (length(common_samples) == 0) {\n",
    "    stop(\"⚠️ No overlapping samples found between raw and decontaminated data.\")\n",
    "  }\n",
    "\n",
    "  # Subset both datasets to only the common samples\n",
    "  raw_counts <- raw_counts[common_samples, , drop = FALSE]\n",
    "  decontaminated_counts <- decontaminated_counts[common_samples, , drop = FALSE]\n",
    "\n",
    "  # Compute total reads before & after SCRuB\n",
    "  total_reads_before <- rowSums(raw_counts, na.rm = TRUE)\n",
    "  total_reads_after <- rowSums(decontaminated_counts, na.rm = TRUE)\n",
    "\n",
    "  # Compute proportion of reads removed per sample\n",
    "  proportion_removed <- (total_reads_before - total_reads_after) / total_reads_before\n",
    "\n",
    "  # Create dataframe for ggplot\n",
    "  df <- data.frame(\n",
    "    Sample = common_samples,\n",
    "    Proportion_Removed = proportion_removed\n",
    "  )\n",
    "\n",
    "  # Ensure we have valid data to plot\n",
    "  if (nrow(df) == 0) {\n",
    "    stop(\"⚠️ No valid read reduction data available for plotting.\")\n",
    "  }\n",
    "\n",
    "  # Box plot of proportion removed\n",
    "  ggplot(df, aes(x = \"\", y = Proportion_Removed)) +\n",
    "    geom_boxplot(fill = \"red\", alpha = 0.5) +\n",
    "    geom_jitter(width = 0.1, alpha = 0.5, color = \"black\") +\n",
    "    labs(\n",
    "      title = \"Proportion of Reads Removed Across Samples\",\n",
    "      x = \"Samples\",\n",
    "      y = \"Proportion of Reads Removed\"\n",
    "    ) +\n",
    "    theme_minimal()\n",
    "}\n",
    "plot_proportion_reads_removed(raw_counts, decontaminated_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha & Beta Diversity ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(phyloseq)\n",
    "library(ggplot2)\n",
    "library(vegan)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "combined_seqtab_orig <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/merged_df_orig.csv\", row.names = 1, check.names = FALSE)\n",
    "combined_taxa <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/taxa.csv\", row.names = 1, check.names = FALSE)\n",
    "features <- colnames(read.csv(\"/Users/mcarrion/Korem_Lab/combined/filtered_seqtab_overlap.csv\", row.names = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine diversity at day 0, preserving independence among samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "day0_rows <- grep(\"-D0\", rownames(combined_seqtab_orig), value = TRUE)\n",
    "day0_seqtab <- combined_seqtab_orig[day0_rows, , drop = FALSE]\n",
    "combined_seqtab_orig <- day0_seqtab  # only consider samples at day 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ref_long <- combined_seqtab_orig[, !(colnames(combined_seqtab_orig) %in% features), drop = FALSE]\n",
    "head(ref_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Add in batch/plate info\n",
    "library(stringr)\n",
    "library(dplyr)\n",
    "\n",
    "# Load file list\n",
    "file_list <- readLines(\"/Users/mcarrion/Korem_Lab/file_list.txt\")\n",
    "\n",
    "# Define directories (batches)\n",
    "val_dirs <- c(\"20241029_16S_FB_ICU\", \"20241127_16S_FB_ICU\", \"20241223_16S_FB_ICU\")\n",
    "orig_dirs <- c(\"20231228_16S_Plate_1\", \"20231228_16S_Plate_2\", \"20231228_16S_Plate_3\", \"20240202_16S_FB\")\n",
    "\n",
    "# Initialize empty batch mapping\n",
    "sample_to_batch <- list()\n",
    "\n",
    "# Cleaning function for filenames\n",
    "clean_sample_name <- function(filename) {\n",
    "  filename <- str_remove(filename, \"_L001\")\n",
    "  filename <- str_remove(filename, \"_R[12]_001\\\\.fastq\\\\.gz$\")\n",
    "  filename <- str_remove(filename, \"_[12]\\\\.fastq\\\\.gz$\")\n",
    "  filename <- str_match(filename, \"([0-9]+[A-Z])\")[,2]\n",
    "  return(filename)\n",
    "}\n",
    "\n",
    "# Process file paths\n",
    "for (filepath in file_list) {\n",
    "  path_parts <- unlist(strsplit(filepath, \"/\"))\n",
    "  dir_name <- path_parts[length(path_parts) - 1]\n",
    "  filename <- path_parts[length(path_parts)]\n",
    "  clean_name <- clean_sample_name(filename)\n",
    "  \n",
    "  if (dir_name %in% val_dirs) {\n",
    "    sample_to_batch[[clean_name]] <- paste0(\"val_\", dir_name)\n",
    "  } else if (dir_name %in% orig_dirs) {\n",
    "    sample_to_batch[[clean_name]] <- paste0(\"orig_\", dir_name)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Convert list to named vector\n",
    "sample_to_batch <- unlist(sample_to_batch)\n",
    "\n",
    "# Add batch column to ref_long\n",
    "ref_long$batch <- sample_to_batch[ref_long$id]\n",
    "\n",
    "# View unmatched rows (if any)\n",
    "unmatched <- ref_long %>% filter(is.na(batch))\n",
    "print(unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(ref_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Add in treatment info\n",
    "ref_long$sample <- rownames(ref_long)\n",
    "intervention_df <- read.csv(\"/Users/mcarrion/Korem_Lab/orig_data/intervention_stats.csv\", check.names = FALSE)\n",
    "colnames(intervention_df)[1] <- \"id\"\n",
    "\n",
    "ref_long <- left_join(ref_long, intervention_df, by = \"id\")\n",
    "colnames(ref_long)[colnames(ref_long) == \"arm\"] <- \"treatment\"\n",
    "rownames(ref_long) <- ref_long$sample\n",
    "head(ref_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Additional Data Cleaning\n",
    "combined_seqtab_orig_filt <- combined_seqtab_orig[, features, drop = FALSE]\n",
    "otu_table_combined <- otu_table(as.matrix(combined_seqtab_orig_filt), taxa_are_rows = FALSE)\n",
    "rownames(otu_table_combined) <- gsub(\"\\\\.+\\\\d+$\", \"\", rownames(otu_table_combined)) # Ensure row names align\n",
    "tax_table_combined <- tax_table(as.matrix(combined_taxa))\n",
    "\n",
    "\n",
    "# Convert to sample_data object\n",
    "sample_data_combined <- sample_data(ref_long)\n",
    "\n",
    "# Only consider common rows between the two tables\n",
    "common_samples <- intersect(rownames(otu_table_combined), rownames(sample_data_combined))\n",
    "otu_table_combined <- otu_table_combined[common_samples, , drop=FALSE]\n",
    "otu_table_combined[is.na(otu_table_combined)] <- 0\n",
    "sample_data_combined <- sample_data_combined[common_samples, , drop=FALSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "physeq_combined <- phyloseq(otu_table_combined, tax_table_combined, sample_data_combined)\n",
    "colnames(sample_data(physeq_combined)) # to be used in the below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_richness(physeq_combined, x=\"death\", measures=c(\"Shannon\", \"Simpson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Build phyloseq object\n",
    "physeq_combined <- phyloseq(\n",
    "  otu_table(otu_table_combined, taxa_are_rows = FALSE),\n",
    "  tax_table_combined,\n",
    "  sample_data_combined\n",
    ")\n",
    "\n",
    "# Estimate alpha diversity\n",
    "alpha_df <- estimate_richness(physeq_combined, measures = \"Shannon\")\n",
    "alpha_df$sample <- rownames(alpha_df)\n",
    "\n",
    "# Merge with sample metadata\n",
    "alpha_df <- left_join(alpha_df, as.data.frame(sample_data_combined), by = \"sample\")\n",
    "\n",
    "# Filter for day 0 only\n",
    "alpha_df_day0 <- alpha_df\n",
    "\n",
    "# Plot boxplot\n",
    "ggplot(alpha_df_day0, aes(x = factor(infection), y = Shannon, fill = factor(infection))) +\n",
    "  geom_boxplot(outlier.shape = NA, alpha = 0.6) +\n",
    "  geom_jitter(width = 0.2, alpha = 0.5, color = \"black\") +\n",
    "  scale_x_discrete(labels = c(\"No Infection\", \"Infection\")) +\n",
    "  labs(\n",
    "    x = \"Infection Status\",\n",
    "    y = \"Shannon Diversity\",\n",
    "    title = \"Shannon Diversity at ICU Admission\"\n",
    "  ) +\n",
    "  theme_minimal(base_size = 20) +\n",
    "  theme(\n",
    "\tplot.title = element_text(hjust = 0.5),\n",
    "\taxis.text.x = element_text(size = 23)  # Increase x-axis label size\n",
    "\t) +\n",
    "  guides(fill = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "wilcox.test(Shannon ~ infection, data = alpha_df_day0)$p.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Compute p-value\n",
    "p_val <- wilcox.test(Shannon ~ death, data = alpha_df_day0)$p.value\n",
    "p_label <- paste0(\"p = \", signif(p_val, 2))\n",
    "\n",
    "# Set y-position for annotation\n",
    "y_max <- max(alpha_df_day0$Shannon, na.rm = TRUE)\n",
    "y_line <- y_max + 0.1\n",
    "y_text <- y_max + 0.2\n",
    "\n",
    "# Plot\n",
    "ggplot(alpha_df_day0, aes(x = factor(death), y = Shannon, fill = factor(death))) +\n",
    "  geom_boxplot(outlier.shape = NA, alpha = 0.6) +\n",
    "  geom_jitter(width = 0.2, alpha = 0.5, color = \"black\") +\n",
    "  scale_x_discrete(labels = c(\"Survived\", \"Died\")) +\n",
    "  labs(\n",
    "    x = \"Mortality Status\",\n",
    "    y = \"Shannon Diversity at ICU Admission\"\n",
    "  ) +\n",
    "  ggtitle(\"Shannon diversity at ICU admission does not differ by mortality status\") +\n",
    "  theme_minimal(base_size = 18) +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 15),\n",
    "    axis.text.x = element_text(size = 16),\n",
    "    axis.title = element_text(size = 16)\n",
    "  ) +\n",
    "  guides(fill = FALSE) +\n",
    "  annotate(\"segment\", x = 1, xend = 2, y = y_line, yend = y_line, size = 0.6) +\n",
    "  annotate(\"text\", x = 1.5, y = y_text, label = p_label, size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(alpha_df_day0, aes(x = factor(death), y = Shannon, fill = factor(death))) +\n",
    "  geom_boxplot(outlier.shape = NA, alpha = 0.6) +\n",
    "  geom_jitter(width = 0.2, alpha = 0.5, color = \"black\") +\n",
    "  scale_x_discrete(labels = c(\"Survived\", \"Died\")) +\n",
    "  labs(\n",
    "    x = \"Mortality Status\",\n",
    "    y = \"Shannon Diversity\",\n",
    "    title = \"Shannon Diversity at ICU Admission\"\n",
    "  ) +\n",
    "  theme_minimal(base_size = 20) +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5),\n",
    "    axis.text.x = element_text(size = 23)  # Increase x-axis label size\n",
    "\n",
    "  ) +\n",
    "  guides(fill = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "wilcox.test(Shannon ~ death, data = alpha_df_day0)$p.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dist_bc <- phyloseq::distance(physeq_combined, method=\"bray\")\n",
    "ord_nmds <- ordinate(physeq_combined, method=\"NMDS\", distance=\"bray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_ordination(physeq_combined, ord_nmds, color=\"death\") +\n",
    "  ggtitle(\"NMDS - Bray-Curtis\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Convert infection and death to labeled factors\n",
    "sample_data(physeq_combined)$infection <- factor(\n",
    "  sample_data(physeq_combined)$infection, levels = c(0, 1), labels = c(\"No Infection\", \"Infection\")\n",
    ")\n",
    "\n",
    "sample_data(physeq_combined)$death <- factor(\n",
    "  sample_data(physeq_combined)$death, levels = c(0, 1), labels = c(\"Survived\", \"Died\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Common theme settings using variables\n",
    "custom_theme <- theme_minimal(base_size = font_main) +\n",
    "  theme(\n",
    "    text = element_text(size = font_main),\n",
    "    axis.title = element_text(size = font_main),\n",
    "    axis.text = element_text(size = font_secondary),\n",
    "    legend.title = element_text(size = font_main),\n",
    "    legend.text = element_text(size = font_secondary),\n",
    "    plot.title = element_text(hjust = 0.5, size = font_main + 1, face = \"bold\")\n",
    "  )\n",
    "\n",
    "\n",
    "# Extract axis variance (optional)\n",
    "eig_vals <- ord_pcoa$values$Relative_eig * 100  # percentage variance\n",
    "\n",
    "# Format axis labels with variance explained\n",
    "axis1_label <- sprintf(\"PCoA Axis 1 (%.1f%%)\", eig_vals[1])\n",
    "axis2_label <- sprintf(\"PCoA Axis 2 (%.1f%%)\", eig_vals[2])\n",
    "\n",
    "# Panel A – Infection\n",
    "p2a <- plot_ordination(physeq_combined, ord_pcoa, color = \"infection\") +\n",
    "  geom_point(size = 3, alpha = 0.8) +\n",
    "  labs(\n",
    "    title = expression(beta*\"-Diversity Does Not Distinguish Infection Groups\"),\n",
    "    x = axis1_label,\n",
    "    y = axis2_label,\n",
    "    color = \"Infection\"\n",
    "  ) +\n",
    "  custom_theme\n",
    "\n",
    "# Panel B – Mortality\n",
    "p2b <- plot_ordination(physeq_combined, ord_pcoa, color = \"death\") +\n",
    "  geom_point(size = 3, alpha = 0.8) +\n",
    "  labs(\n",
    "    title = expression(beta*\"-Diversity Does Not Distinguish Mortality Groups\"),\n",
    "    x = axis1_label,\n",
    "    y = axis2_label,\n",
    "    color = \"Mortality\"\n",
    "  ) +\n",
    "  custom_theme\n",
    "  p2\n",
    "  #p2b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if there are inherent batch differences / differences in plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Try PCOA \n",
    "ord_pcoa <- ordinate(physeq_combined, method=\"PCoA\", distance=\"bray\")\n",
    "plot_ordination(physeq_combined, ord_pcoa, color=\"batch\") +\n",
    "  ggtitle(\"PCoA - Bray-Curtis\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(vegan)\n",
    "\n",
    "# Compute Bray-Curtis distance matrix\n",
    "dist_mat <- phyloseq::distance(physeq_combined, method = \"bray\")\n",
    "\n",
    "# Extract metadata\n",
    "meta <- as(sample_data(physeq_combined), \"data.frame\")\n",
    "\n",
    "# Run PERMANOVA\n",
    "adonis2(dist_mat ~ batch, data = meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate alpha diversity\n",
    "alpha_df <- estimate_richness(physeq_combined, measures = c(\"Shannon\", \"Simpson\"))\n",
    "alpha_df$death <- sample_data(physeq_combined)$death  # add metadata\n",
    "\n",
    "# Wilcoxon test (for binary outcome)\n",
    "wilcox.test(Shannon ~ death, data = alpha_df)\n",
    "\n",
    "# OR Kruskal-Wallis (if death has >2 levels)\n",
    "kruskal.test(Shannon ~ death, data = alpha_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(vegan)\n",
    "\n",
    "# Compute Bray-Curtis distance matrix\n",
    "dist_mat <- phyloseq::distance(physeq_combined, method = \"bray\")\n",
    "\n",
    "# Extract metadata\n",
    "meta <- as(sample_data(physeq_combined), \"data.frame\")\n",
    "\n",
    "# Run PERMANOVA\n",
    "adonis2(dist_mat ~ death, data = meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine diversity over time, stratifying by sampleID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "combined_seqtab_orig <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/merged_df_orig.csv\", row.names = 1, check.names = FALSE)\n",
    "combined_taxa <- read.csv(\"/Users/mcarrion/Korem_Lab/combined/taxa.csv\", row.names = 1, check.names = FALSE)\n",
    "features <- colnames(read.csv(\"/Users/mcarrion/Korem_Lab/combined/filtered_seqtab_overlap.csv\", row.names = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ref_long <- combined_seqtab_orig[, !(colnames(combined_seqtab_orig) %in% features), drop = FALSE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Additional Data Cleaning\n",
    "combined_seqtab_orig_filt <- combined_seqtab_orig[, features, drop = FALSE]\n",
    "otu_table_combined <- otu_table(as.matrix(combined_seqtab_orig_filt), taxa_are_rows = FALSE)\n",
    "rownames(otu_table_combined) <- gsub(\"\\\\.+\\\\d+$\", \"\", rownames(otu_table_combined)) # Ensure row names align\n",
    "tax_table_combined <- tax_table(as.matrix(combined_taxa))\n",
    "\n",
    "\n",
    "# Convert to sample_data object\n",
    "sample_data_combined <- sample_data(ref_long)\n",
    "\n",
    "# Only consider common rows between the two tables\n",
    "common_samples <- intersect(rownames(otu_table_combined), rownames(sample_data_combined))\n",
    "otu_table_combined <- otu_table_combined[common_samples, , drop=FALSE]\n",
    "otu_table_combined[is.na(otu_table_combined)] <- 0\n",
    "sample_data_combined <- sample_data_combined[common_samples, , drop=FALSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "physeq_combined <- phyloseq(otu_table_combined, tax_table_combined, sample_data_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(lme4)\n",
    "library(lmerTest)\n",
    "\n",
    "# Estimate alpha diversity\n",
    "alpha_df <- estimate_richness(physeq_combined, measures = \"Shannon\")\n",
    "meta <- as(sample_data(physeq_combined), \"data.frame\")\n",
    "meta$subjectID <- sub(\"-D.*\", \"\", rownames(meta))             # everything before \"-D\"\n",
    "meta$day <- as.numeric(gsub(\".*-D(\\\\d+)\", \"\\\\1\", rownames(meta)))  # number after \"-D\"\n",
    "\n",
    "# Merge richness with metadata\n",
    "alpha_df$subjectID <- meta$subjectID\n",
    "alpha_df$day <- meta$day\n",
    "alpha_df$infection <- meta$infection\n",
    "alpha_df$death <- meta$death\n",
    "\n",
    "\n",
    "# Fit linear mixed model: Shannon ~ day * death + (1 | subjectID)\n",
    "lmm <- lmer(Shannon ~ day * death + (1 | subjectID), data = alpha_df)\n",
    "summary(lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(123)  # for reproducibility\n",
    "\n",
    "otu_table_matrix <- as(otu_table(physeq_combined), \"matrix\")\n",
    "\n",
    "# If taxa are rows, transpose\n",
    "if (taxa_are_rows(physeq_combined)) {\n",
    "  otu_table_matrix <- t(otu_table_matrix)\n",
    "}\n",
    "\n",
    "# Compute Bray-Curtis distance\n",
    "bray_dist <- vegdist(otu_table_matrix, method = \"bray\")\n",
    "\n",
    "# Distance matrix (e.g., Bray-Curtis)\n",
    "dist_mat <- phyloseq::distance(physeq_combined, method = \"bray\")\n",
    "\n",
    "adonis2_result <- adonis2(\n",
    "  bray_dist ~ infection,\n",
    "  data = meta,\n",
    "  permutations = 999,\n",
    "  strata = meta$subjectID  # block by subject\n",
    ")\n",
    "\n",
    "print(adonis2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure infection is treated as a categorical variable\n",
    "alpha_df$infection <- factor(alpha_df$infection, levels = c(0, 1), labels = c(\"No Infection\", \"Infection\"))\n",
    "\n",
    "ggplot(alpha_df, aes(x = day, y = Shannon, color = infection, group = infection)) +\n",
    "  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, size = 1.2) +\n",
    "  coord_cartesian(ylim = c(0, 5)) +\n",
    "  labs(\n",
    "    title = \"Shannon Diversity Over Time by Infection Status\",\n",
    "    x = \"Day Post-ICU Admission\",\n",
    "    y = \"Shannon Diversity\",\n",
    "    color = \"Infection Status\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(alpha_df, aes(x = infection, y = Shannon, fill = infection)) +\n",
    "  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n",
    "  geom_jitter(width = 0.2, alpha = 0.5, color = \"black\") +\n",
    "  coord_cartesian(ylim = c(0, 5)) +\n",
    "  labs(\n",
    "    title = \"Overall Shannon Diversity by Infection Status\",\n",
    "    x = \"Infection Status\",\n",
    "    y = \"Shannon Diversity\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5),\n",
    "    legend.position = \"none\"\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
